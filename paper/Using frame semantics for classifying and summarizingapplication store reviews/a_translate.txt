フレームセマンティクスを用いたアプリケーションストアのレビューの分類と要約

概要
近年、モバイルアプリケーションストアのユーザーレビューを分類・要約するために、テキストマイニング技術が採用されています。しかし，ユーザーが作成したオンラインテキストデータは多様で非構造化されているため，テキストベースのレビューマイニング技術では，過度に複雑なモデルが作成され，オーバーフィッティングが発生しやすいという問題がある．本論文では、フレームセマンティクスに基づいたアプリレビューマイニングのための新しいアプローチを提案します。意味フレームは、生のテキスト（個々の単語）から、より抽象的なシナリオ（コンテキスト）に一般化するのに役立ちます。このようにテキストを低次元で表現することで、レビューマイニング技術の予測能力を高め、オーバーフィッティングの可能性を低減することが期待されます。具体的には、本稿では2つの分析を行います。1つ目は、情報量の多いユーザーレビューを、実行可能なソフトウェアメンテナンス要求の様々なカテゴリに分類する際のセマンティックフレームの性能を調査します。次に、情報量の多いレビューの簡潔で代表的な要約を生成するために、複数の要約アルゴリズムを提案し、その性能を評価します。実験分析には、アプリストアのレビューを対象とした3つの異なるデータセットを用いました。その結果、セマンティックフレームは、効率的で正確なレビュー分類プロセスを可能にすることが分かりました。しかし、レビューの要約タスクにおいては、フレームベースの要約よりもテキストベースの要約の方がより包括的な要約を生成することが示されました。最後に、本研究で調査したアルゴリズムを実装したレビュー分類・要約スイートであるMARC 2.0を紹介します。

1 はじめに
過去10年間におけるモバイル産業の急速な成長は，ソフトウェアの生産と消費のあり方にパラダイムシフトをもたらした(Petsas et al. 2013)．モバイル技術がより身近になるにつれ、より多くの消費者が日々のコンピューティング活動をスマートフォンやタブレットに移行しています。このような成長に対応するために、モバイルアプリケーション（アプリ）ストア（Google PlayやApple App Storeなど）が、オンライン配信プラットフォームの新しいモデルとして登場しました（Basole and Karla 2012）。これらのストアは、過去5年間で規模が拡大し、何百万ものアプリをホストしており、ソフトウェアのエンドユーザーが実質的に無制限に選択できるようになっています。例えば、2017年3月の時点で、Apple App Storeだけでも約220万本のアクティブなアプリが報告されており、1日あたり1000本以上のペースで増加しています1。

従来のオンラインマーケット（AmazonやeBayなど）と同様に、モバイルアプリストアでは、顧客がアプリの使用感をテキストのレビューや星の評価という形で共有することができます。このユニークなユーザーフィードバックのチャネルにより，アプリ開発者は，自社のソフトウェアを使用する大規模かつ異種のエンドユーザー集団の意見を直接モニターできるという，これまでにない機会を得ることができました（Pagano and Maalej 2013）．実際、アプリストアのレビューの大規模なデータセットを分析すると、最新の技術情報が相当量含まれていることがわかります。このような情報は、アプリ開発者が、競争が激しく変動の激しい市場でアプリを維持・継続するために活用することができます（Pagano and Maalej 2013）。ソフトウェア制作プロセスへのユーザーの参加は、ソフトウェアの成功に大きく寄与するという考え方が根底にあります（Bano and Zowghi 2015）。

アプリストアのフィードバックの技術的および市場的な価値を認識して、有益なユーザーレビューを自動的に捕捉して分類するための多数の手法やツールが文献で提案されています（Carre´no and Winbladh 2013; Chen et al.2014; Maalej and Nabil 2015; Pagano and Maalej 2013; Villarroel et al. 2016）。一般的に、アプリストアのマイニング技術は、ユーザーレビューのテキスト属性に依存して、機能要求やバグ報告を含む、きめ細かいソフトウェアメンテナンス要求に分類します。このような技術は、特定の指標となる用語（「クラッシュ」や「バグ」など）の有無を検出するものから、テキストモデリングや分類技術に依存した、より計算量の多い手法まで多岐にわたります（Carre´no and Winbladh 2013; Guzman and Maalej 2014; Maalej and Nabil 2015; Panichella et al. しかし、これらの技術はまずまずの精度を示しているものの、一般的にいくつかの欠点を抱えています。例えば、ユーザーは口語的な用語や新語を含む非公式な言語を使ってレビューを表現する傾向があります。このような幅広い単語を使用すると、テキスト分類モデルが複雑になり、オーバーフィッティングの問題が発生することがあります。特に、自然言語はオンラインで急速に進化するため、ある時点で収集された語彙を用いて学習された分類器は、見たことのないレビューを正確に一般化できない可能性があります（McCallum and Nigam 1998）。

これらの課題を解決するために、本稿では、モバイルアプリストアのユーザーレビューをマイニングし、分類するための新しい意味的認識アプローチを提案します。このアプローチは、セマンティックロールラベリング（SRL）という概念に基づいています。SRLの主な前提は、単語はフレームと呼ばれる意味的なクラスにグループ化できるということです。意味フレームは、文の中で発生するイベントを、その参加者（人や物など）とともに記述します。その目的は、文の意味をより高い抽象度で捉えることにあります。具体的には、文章中の単語やフレーズに様々なフレーム要素（または役割）をアノテーションすることで、特定の文章からシナリオまでを一般化することができる。このようなアノテーションは、FrameNetプロジェクト(Baker et al. 1998)を使用して生成することができます。FrameNetは、意味的フレームとその役割のオンライン語彙リポジトリを提供しています。

SRLとフレームセマンティクスは、金融ニュース記事のテキストコンテンツを分析することによる株式市場の動きの予測(Xie et al. 2013)、非構造化テキストからのソーシャルネットワークの抽出(Agarwal et al. 2014)、質問応答タスク(Shen and Lapata 2007)、政治討論におけるスタンスの分類(Hasa and Ng 2013)など、さまざまなテキストマイニングタスクで利用されてきた。このような研究の流れを受けて、本稿では、アプリストアのレビューマイニングの基本的なアルゴリズムをサポートするフレームセマンティクスの性能を調査します。本論文の目的は、有益なユーザーレビューを識別、分類、要約して、実行可能なソフトウェア保守要求の異なるグループに分類するための、軽量で正確な一連のアルゴリズムを説明することです。我々の分析は、幅広いアプリケーションドメインからサンプリングされたアプリレビューのデータセットを使用して行われます（Chen et al. 2014; Jha and Mahmoud 2017b; Maalej and Nabil 2015）。具体的には、本稿で紹介する作業は、Jha and Mahmoud (2017b)における我々の以前の作業を以下のように拡張する。

- レビューの要約。複数のアプリストアでホストされている人気のあるモバイルアプリは、多くの場合、何千もの有益なレビューを生成します。このような大規模な、そして典型的には冗長な量の生のユーザーレビューを開発者に提示すると、混乱を招く可能性があります。このため，レビューに含まれる最も重要な問題を特定して要約し，より効果的なデータ探索プロセスを可能にする自動化された手法が必要とされています．このニーズに対応するため、我々はモバイルアプリのレビューの文脈において、様々なテキスト要約アルゴリズムの性能を評価する。我々のアルゴリズムセットは、Hybrid TFとHybrid TF.IDF (Inouye and Kalita 2011)、SumBasic (Nenkova and Vanderwende 2005)、およびLexRank (Erkan and Radev 2004)で構成される。これらのアルゴリズム（LexRankを除く）は、私たちの以前の研究で、いくつかの人気ソフトウェアシステムのTwitterフィードで利用可能なソフトウェア関連のツイートを要約するために使用されました（Williams and Mahmoud 2017）。

- ツールのサポートです。Jha and Mahmoud (2017a)では、我々のアルゴリズムを実装したMobile App Review ClassifierであるMARCを紹介しました。この拡張版では、強化されたGUI、要約エンジン、より正確で効率的な分類エンジンを備えたMARCの新リリースであるMARC 2.0を紹介します。

- 本文とディスカッションを強化しました。我々の提案した方法、実験的な分析、関連する研究、有効性に対する脅威などについて、より詳細な議論を加えることで、論文の内容を大幅に強化しました。

本論文の残りの部分は以下のように構成されています。セクション2では、FrameNetプロジェクトとセマンティックフレームの概念を紹介します。セクション3では、我々のレビュー分類プロセスについて説明します。セクション4では、我々のレビュー要約アルゴリズムを説明し、評価します。セクション5では、MARC 2.0を紹介します。セクション6では、本稿の研究に関連する主要な研究をレビューします。セクション7では、有効性に対する主な脅威について議論する。最後に、セクション8では、本論文の結論と今後の展望について述べる。

2 フレームセマンティクス
カリフォルニア州バークレーにあるInternational Computer Science Instituteが運営するFrameNetプロジェクト(Baker et al. 1998)は、Frame Semantics(Fillmore 1976)の理論に基づいて手動でアノテーションされた文の巨大な機械可読データベースを提供しています。この理論は、語彙項目（述語）の意味は、フレームと呼ばれるより大きな概念的なチャンクに関して、最もよく定義されるというものです。技術的には、FrameNet2プロジェクトは、文の中の重要なフレーム、そのフレーム要素、および語彙ユニットを識別するために活動しています。セマンティック・フレーム（または単にフレーム）は、様々な要素を含む状況（イベント、アクション）の概略的な表現として記述することができます。フレーム要素(FE)は、フレームによって記述されたアクションにおける参加者の実体または意味的役割として定義できる。語彙単位(LU)とは、基本的に、異なるフレーム要素を想起させる言葉のことである。例えば、「COMMERCE BUY」というフレームは、買い手と売り手がお金や商品を交換する基本的な商取引を記述している。このフレームには、コアフレーム要素であるbuy（buyなどの語彙単位で想起される）とgoodsがある。コア・フレームとは、フレームが発生するために必要な要素である。COMMERCE BUYフレームには、place、purpose、seller、timeなどのいくつかのnon-core FEもあります。

図1は、"John bought a car from Kristina in June. "という文の意味フレームCOMMERCE BUYの下でのセマンティック・アノテーションを示しています。この文には、John, car, Kristina, Juneの各語彙によって、buyer, goods, seller, timeの各フレーム要素が含まれている。別の例として、表1に示す「He traveled to Germany to buy a car」という文があります。この文は、TRAVEL、COMMERCE BUY、VEHICLEという意味フレームでアノテーションされている。TRAVELの意味フレームには、旅行者と目標という要素があり、それぞれheとto Germanyという単語によって喚起される。COMMERCE BUY」フレームには、「he」と「car」という単語でそれぞれ表現される「buyer」と「goods」という要素があり、「VEHICLE」フレームには、「car」という単語で表現される「vehicle」という要素があります。

このようなユニークな形のセマンティックアノテーションは、様々なテキスト処理タスクをサポートするために利用できる貴重な知識源となります。例えば、FrameNetデータベースは、テキストの意味的な分類(Fleischman et al. 2003)、質問応答(Sinha 2008)、情報抽出(Moschitti et al. 2003)などのタスクに使用されています。このような研究の流れに沿って、私たちはFrameNetプロジェクトを利用して、アプリストアのユーザーレビューのマイニングの問題に取り組みます。私たちは、FrameNetのタグ付けによって、個々のユーザーレビューの意味を深く理解できるようになることを期待しています。これにより、より正確なアプリレビューのマイニングアルゴリズムを生成することができます。例えば、写真共有アプリImgurのレビューから抽出された「I can't see the pictures fix it please!!!」という文章を考えてみましょう。この文章にFrameNetを使ってタグ付けすると、次のようなフレームができます。

I [can't]capability [see]GRASP the [pictures]PHYSICAL ARTWORKS [fix]PREDICAMENT it [please]STIMULUS FOCUS.

この例で重要な意味フレームはPREDICAMENTで、「経験者が望ましくない状況にあり、その原因が表現されることもある」という状況を指します。このフレームは、problem、trouble、jamなどの他の単語によっても呼び起こすことができます。一般に、不都合な状況はすべてこのフレームを呼び起こす可能性があります。分類の観点からは、このフレームは、バグレポートを予測するために使用できる特徴を表しています。

もう1つの例は、Gmailアプリの2つの異なるレビューから抽出された2つのレビュー文「I wish you could add a functional to use this app with any POP3 mailboxes. この2つの文章は、すべてのPOP3メールボックスに対応するための機能要求を記述しているという同じメッセージを伝えていますが、用語は異なっています。この2つの文章にFrameNetを用いてタグ付けすると、以下のような表現になります。

私は、あなたがこのアプリを[any]quantity POP3メールボックスで[use]使用するための機能性を[could]capability [add]statementしてくれることを[wish]desiringしています。

私は、Gmailを[すべての]QUANTITY POP3メールボックスで使用することができるように[できる]CAPABILITYになりたいと思っています。

第1文では、wish、could、add、use、anyという言葉が、それぞれDESIRING、CAPABILITY、STATEMENT、USING、QUANTITYというフレームを喚起しています。2文目ではwired, able, use, and allがそれぞれDESIRING, CAPABILITY, USING, and QUANTITYのフレームを喚起している。この例は、特定の文脈で同じ意味を持つ異なる単語によって、類似したフレームが喚起されることを示しています。例えば、上の2つの文章では、wishとwantedという言葉は、特定の文脈において同じ意味を共有する2つの異なる言葉であり、したがって、これらは両方ともDESIRINGというフレームを喚起する。同様に、couldとableという単語は、両方の文章で意味フレームのcapabilityを喚起します。

このように意味を抽象化することで、テキスト分類器の予測能力が向上することが期待される。特に、テキストの分類タスクでは、テキストの個々の単語が個別の分類特徴として扱われ、入力テキストがその単語の順序付けられていないベクトルとして表現される。このアプローチは、Bag-of-Words（BOW）分類として知られており、テキスト中の特定の指標となる用語の有無に依存して判断を下します。例えば、アプリレビューの分類では、{bug, crash, fix, problem, issue, defect, solve, trouble}などの単語は、バグ報告のレビューに関連する傾向があり、{add, please, would, hope, improve, miss, need, prefer, suggest, want, wish}などの単語は、一般的に機能要求やユーザーの要望に関連します（Maalej and Nabil 2015）。このような単語は，テキスト分類器によって，入力テキストを特定のラベルに分類するために使用される．対照的に、本稿で紹介するアプローチは、Bagof-Frames（BOF）アプローチと表現できる。特に、各レビューに生成されたフレームは、その単語ではなく、テキストを表現するための分類特徴として使用されます（すなわち、フレームのベクトル）。我々の仮定は、データのBOF表現は、より低次元であり、したがって、より正確なモデルを生成する可能性があるということです。以下では、レビューの分類と要約を含む、2つの基本的なレビューマイニングタスクにおいて、セマンティックフレームを使用することの影響を検証します。

3 アプリレビューの分類

このフェーズでは、アプリレビューの分類タスクで一般的に使用されるテキスト分類器の精度に、フレームの意味論を使用することの影響を検証します。以下では、分析に使用したデータセット、データの分類に使用した分類アルゴリズム、異なる分類構成の下でこれらのアルゴリズムのパフォーマンスを評価するために使用した指標など、実験的なセットアップについて説明します。

3.1 実験データセット

レビューの約25%はMaalej and Nabil (Maalej and Nabil 2015)が収集したデータからランダムに4サンプリングし、50%はChen et al.のデータセット(Chen et al. 2014)からサンプリングしています。残りの25％のレビューは、iOSアプリ「CreditKarma」、「FitBit」、「Gmail」からローカルに収集しました。このような多様なデータを使用することで，一般的にアプリサンプリング問題として知られているサンプリングバイアスの可能性を低減し，結果の内的・外的妥当性を高めることができました（Martin et al.2015）．

ローカルデータセットでは，iOSアプリストアのRSSフィードジェネレータを用いて，各アプリの最新のユーザーレビューを抽出しました。これらのレビューは，著者と外部審査員が手動で，機能要求，バグ報告，その他に分類した．各レビューの最終クラスの決定には，多数決が用いられた．さらに，Maalej and Nabil (2015) および Chen et al. (2014) からサンプリングしたデータを研究者が再検討し，その分類が我々の分類スキームと一致していることを確認しました。例えば、Maalej and Nabil (2015) のレビュー「Just un install and reinstall Works Awesome now Love this app probably best ever！」は、そのタイトル（「Crash and will not open FIX」）からバグレポートに分類しました。今回の分析では、レビューのタイトルは考慮しませんでした。そのため、このレビューの分類は、uninformative（つまり、それ以外）に変更しました。合計すると，我々の分類が外部データセットの元々の分類と一致しなかったのは，レビュー総数の3％未満であった．

いくつかのケースでは，いくつかのレビューが各審査員によって異なるラベルを付けられ，さらに審査員間で議論しても明確なラベルにはならなかった．例えば，「ゲームが好きだが，あまり速くない無線LANでプレイするのは少し難しい」というレビューは，1人の審査員がバグレポート，2人目の審査員が機能要望，3人目の審査員がそれ以外と分類しました．審査員の間で議論した結果、最終的なラベルの付け方に合意が得られなかったため、レビューは削除されました。合計で13のインスタンスがデータセットから取り除かれた．表2は，データセットの特徴をまとめたもので，各ソースから収集したバグレポート，機能要求，その他のインスタンスの数を示している．

3.2 分類法

データの分類には，サポートベクターマシン（SVM）とナイーブベイズ（NB）を使用しました．この2つのアルゴリズムは、テキスト分類の研究でよく使われており(Cai and Hofmann 2004; Dumais and Chen 2000; Joachims 1998; Kim et al. 2006; Sebastiani 2002)、短文の分類タスクにおいて他の分類器よりも優れた性能を示すことが報告されている(例, Twitterデータ(Guzman et al. 2016; Williams and Mahmoud 2017)、YouTubeコメント(Poch´e et al. 2017)、アプリのユーザーレビュー(Guzman et al. 2015; Maalej and Nabil 2015; Panichella et al. 2015; Wang and Manning 2012)）。) 以下では、これらのアルゴリズムをより詳細に説明します。

- サポートベクターマシン（SVM）。
SVMは，多次元データ空間のパターンを認識するために用いられる教師付き機械学習アルゴリズムである(Burges 1998)．SVMは，データ中の線形分離可能なパターンに対する最適な超平面を見つけようとし，分離超平面の周りのマージンを最大化します．技術的には，サポートベクターは学習セットの重要な要素であり，これを除去すると分割超平面の位置が変わってしまいます．SVMは、入力ベクトルをNd次元空間にマッピングし、定義された超平面のどちら側に点があるかを判断することでデータを分類します。サポートベクター分類器は、高次元で疎なテキストの分類タスクに有効であることが経験的に示されている(Joachims 1998)。

- ナイーブ・ベイズ（NB）。
NBは、ベイズの定理（Langley et al.1992）に基づいた、シンプルで効率的な線形確率的分類器です。NBは、データの属性値がクラスに応じて互いに独立であるという条件付き独立性の仮定に基づいています。テキスト分類の文脈では、モデルの特徴は、テキストアーチファクトの個々の単語です。このようなデータは、通常、2次元の単語×文書行列を用いて表現されます。行列のエントリーi,jは、文書diに単語wjが含まれているかどうかを示すバイナリ値（すなわち{0,1}）、または文書diに現れる単語wjの相対的な頻度のいずれかになります（McCallum and Nigam 1998）。

3.3 実装と分類設定

分析には、さまざまな機械学習および分類技術を実装したデータマイニングソフトウェアであるWeka5を使用しています。SVMは、John Plattのサポートベクトル分類器を学習するための逐次最小最適化アルゴリズムを実装したWekaのSMOを通じて起動される(Platt 1998)。分類器の評価には、10-fold cross validationを使用しています。この評価方法では，データセットを10分割し，それぞれの分割で90%のインスタンスを学習セット，残りの10%を評価セットとしている．評価セットは、その和がデータセット全体となるように選択されます。この手法の利点は，ホールドアウト法(データの70%をトレーニングに，30%をテストに使用する)などの単純な手法に比べて，結果のばらつきが著しく少ないことである(Kohavi 1995)．

データの BOF 表現の生成（レビュー文のアノテーション）には， 確率的フレーム意味解析器である SEMAFOR6 を使用しています（Das et al.2010）．SEMAFORは、英語の文章を自動的に処理して、特別なXML形式の意味的注釈を生成します。私たちは、XML出力から各注釈文の意味フレームを抽出する特別なパーサーを作成しました。

BOW分析では、Wekaで提供されているIteratedLovinsStemmerを使用して、データセット内のレビューをステム処理しました（Lovins 1968）。語幹処理は、単語をその形態学的な根源に還元します。これにより、単語の基本形を1つだけ考慮するため、特徴（単語）の数が減ります。最も一般的な単語（すべてのレビューに登場する単語）と、1つのレビューにしか登場しない単語をデータから削除しました。これらの単語は、一般化できる情報を持っている可能性が非常に低いからです。英語のストップワードはデータから削除しませんでした。この判断は、これらの単語（would、should、willなど）のいくつかは、機能リクエストのレビューの重要な特徴的情報を運ぶことができるという以前の観察に基づいています（Maalej and Nabil 2015; Panichella et al. 例えば、これらのリクエストのいくつかは、"would you"、"could you please"、"why don't you "などのフレーズで始まります。そのため、このような単語を削除すると、分類器の予測能力が低下する可能性があります。

さらに、我々の分析では、文書中の単語の正規化頻度（TF）を使用するMultinomial NBを使用しています(McCallum and Nigam 1998)。Multinomial NBは、よりロバストなテキスト分類器として知られており、多様性の高い実世界のコーパスにおいて、二値特徴モデル(Multi-variate Bernoulli)を常に上回っています(McCallum and Nigam 1998)。

3.4 評価

Recall、precision、およびF-scoreは、本分析で使用された異なる分類技術の性能を評価するために使用されます。Recallはカバー率の指標です．特定のラベルの下で正しく分類されたインスタンスの数と，実際にそのラベルに属するデータ空間のインスタンスの数との比率を表しています．一方、Precisionは、精度を表す指標です。これは、特定のラベルの下で正しく分類されたインスタンスの数と、そのラベルの下で分類されたインスタンスの総数との比率を表します。形式的には，Aをラベルλに属するデータインスタンスの集合，Bを分類器によってそのラベルに割り当てられたデータインスタンスの集合とすると，リコール（R）とプレシジョン（P）は次のように計算できます．

また，結果の報告には，Fβスコアを使用しています．この指標は，回収率と精度の調和平均を表し，次のようになります．

βについては、精度対リコールの優先順位に応じて、異なる値を用いることができる（Berry 2017; Powers 2014）。例えば、要件トレーサビリティやバグローカリゼーションなどのタスクでは（Huffman-Hayes et al. 2006; Khatiwada et al. 2018）、省略のエラー（偽陰性）は、委託のエラー（偽陽性）よりも対処が難しい。このようなタスクでは、一般的に、精度よりもリコールを重視するF2スコアが使用されます。我々の分析では、F1（β＝1）を使用しています。これは、両方のタイプの検索エラー（省略と委託）が労力の節約に同じ影響を与えると仮定しているからです。この仮定は、レビューの数が比較的多い（数千件まで）場合、自動化されたサポートが必要であるという事実に基づいています。そのため、精度が低いと、ユーザーはアウトプットに埋もれている正解を見つけるために、多くの有益でないレビューをかき分けることになります。一方、再現率が低いと、ユーザーは、まったく検索されなかった懸念事項を探すために、さらに多くのレビューを手動で調べることになります。

3.5 結果と考察

分類処理の結果を表3に示します。BOF表現を用いた場合，SVMはFbugs=0.86，Ff eat.=0.74を達成し，NBはFbugs=0.81，Ff eat.=0.70を達成し，NBを上回る結果となりました．BOW表現でも同様の結果となり、SVMはFbugs=0.85、Ff eat.=0.75を達成しましたが、NBはFbugs=0.79、Ff eat.=0.72を達成しました。一般的に，SVMはNBよりも優れており，2つの異なるデータ表現の下でほぼ同じ性能を達成しています．異なる設定の下での精度のばらつきは、精度とリコールのトレードオフに起因すると考えられます（リコールが高いと偽陽性の数が多くなることが多い）。NBと比較してSVMの性能が相対的に優れているのは、オーバーフィッティングを回避する傾向があるためで、これは特徴数に依存しないマージン最大化の固有の動作です(Brusilovsky et al. 2007)。そのため，適切なカーネルを選択すれば，疎なインスタンスを持つ高次 元データ空間にも対応できる可能性がある(Joachims 1998)．適切なカーネル関数の選択は，SVMの汎化能力と予測能力に大きく影響する(Steinwart 2001)．我々の分析では，SVM+BOW分類器の最高性能は正規化ポリカーネルを用いて達成されたが，SVM+BOF分類器はσ=8とω=1のPearson VII関数ベースのユニバーサルカーネル(Puk)を用いて最大値を達成した( ¨Ust¨un et al. 2006)．

分類器の生成能力を評価するために，元のデータセットに含まれていないアプリ（Google Chrome，Facebook，Google Maps）からサンプリングした外部のレビューセットで，分類器の性能をテストしました．データの多様性を高めるために，各アプリから異なる数のレビューをサンプリングしました．元のデータセット（表2）のレビューと同様に，新たにサンプリングしたレビューは，著者と外部審査員が手動で分類しました（セクション3.1参照）．我々の主な目的は，生成された分類器が未経験のデータに対して一般化できるかどうかをテストすることであり，言い換えれば，オーバーフィッティングをテストすることである．自動分類において，オーバーフィッティングとは，分類器が一般的なカテゴリを学習するのではなく，個別のデータインスタンスを学習する（すなわち，学習データをモデル化する）現象を指す．形式的には，モデルMがデータにオーバーフィットするのは，MがM'よりも訓練データに対する誤差が小さくなるような，他のモデルM'が存在する場合である．しかし，M'は，分布全体においてMよりも小さい誤差を持つ(Mitchell 1997)．

オーバーフィッティングをテストするために，表2のデータを用いて生成したオリジナルのモデルを保存し，再読み込みして，テストセットで再評価した．外部テストセットに対する分類器の性能を表5に示す．この結果から，BOF 分類器は，BOW 表現を用いて生成された分類器よりも優れていることがわかる．具体的には，BOF+SVM は，Fbugs = 0.96 と Ff eat. = 0.75 を達成した．一方，BOW表現を用いた分類器の性能は，テストセットに含まれる特徴量の要求に応じて大幅に低下し，SVMではFf eat.=0.54，NBではFf eat.=0.39となり，元のデータセットで達成された性能には及ばなかった．

一般的に、テストデータセットでの結果は、BOW表現で学習したNBとSVMがオーバーフィッティングを起こしていることを示唆しています。この結果は、特徴空間（単語数）が非常に大きいことに起因しています（Joachims 1998）。特徴数が多いと、レビューのベクトル表現(BOW)が非常に疎になります(非ゼロの重みを持つエントリが非常に少ない)。これにより、分類器は一般的な分類カテゴリではなく、特定のデータインスタンスを学習することになります。一方、BOF表現では、特定の単語からより抽象的な意味表現へと抽象度を上げることで、この問題を克服しているようです。分類器が考慮しなければならない特徴の数を減らすことで，オーバーフィッティングの可能性が減り，未見のデータインスタンスに対するより良い一般化につながります．例えば、表6は、我々のオリジナルデータセット（表2）で最も人気のあるフレームを示しています。BOWの学習データセットには「desire」という単語が含まれていませんでした。その結果、BOWテストセットのfeature request "another window is highly desired "は、uninformative (e.g. otherwise) と誤って分類されました。しかし、BOF表現では、desireという単語がDESIRINGというフレームを連想させるため、このレビューは機能要求として正しく分類されました。

特徴量の数が少ないと，オーバーフィッティングの可能性が減るだけでなく，分類器の計算量を減らして学習プロセスを高速化することができます．我々の分析では，BOF表現では分類器の構築に10秒，10倍評価戦略を用いた評価に96秒を要したのに対し，BOW表現では分類器の構築に32秒，評価に293秒を要した．これは、BOWモデルが1592個の単語を用いて構築されたのに対し、BOFモデルの構築には552個のユニークフレームしか用いられなかったことに基づいて説明できます。平均して、データの BOF 表現は、BOW 表現を使用してモデルを構築するのに必要なスペースと時間の 60% を節約することができます。

セマンティックフレームのアプローチでは、FrameNetデータベースをローカルにダウンロードする必要があることを指摘しておくことが重要です。このデータベースは、約500メガバイトの容量を占めています。しかし、このオンラインサービスでは、レビューのセマンティック表現を生成するのに多くの時間を要します。しかし，オンラインサービスでは，レビューの意味的表現を生成するのに，より多くの時間を要します．返されたWebページを解析して，テキストの意味的なフレーム表現を抽出する必要があります．図2は，長さが3，6，9フレームの10個のレビューの意味的表現を抽出するのに必要な時間を示しています．この時間は，読み取りの正確さを確保するために，5回の実行で測定されています．この分析を実行するのは

4 Review Summarization（レビューの要約

分析の第一段階では，有用なユーザーレビューを高い精度で分離することができました．しかし，このような大量かつ冗長な生のレビューを開発者に提示すると，混乱を招く可能性があります．このことから、より効果的なデータ探索プロセスを促進するために、技術的に有益なレビューの中から最も差し迫った問題を特定し、要約する自動化された手法の必要性が強調されています（Sorbo et al. 要約は、テキストコレクションに存在する主なテーマを概説する短くコンパクトな説明と表現することができます（Khabiri et al.2011; Llewellyn et al.2014）。レビュー要約の目的は、多数のユーザーの関心事をいくつかの主要なトピックに同化させることである。

4.1 自動要約

我々の分析のこの段階での主なタスクは、各ユーザーレビューを別のドキュメントとして扱うマルチドキュメント要約問題として説明することができます。マルチドキュメントの要約技術には、抽出的なものと抽象的なものがある。抽象化手法では、テキストコレクションの全体的な内容を説明する、適切な英語の語り口を持つ新規の簡潔な文章を生成します。一方、抽出的な手法は、テキストにすでに存在する特定の文章やキーワードを、テキストコレクション全体の代表として選択するものである。

2.80GHzのCPUと16.0GBのRAMを搭載し、50Mbpsのインターネット速度で動作します。抽象化手法は、抽出された情報に基づいて新規の文章を言い換えるために、重い語彙解析と推論を含むことが多い(Hahn and Mani 2000)。そのため、科学文書やニュース記事など、意味的に豊かで文法的に健全なコーパスに対して有効であることが知られている(Barzilay et al. 1999; Cheung 2008)。しかし、言語学的な観点から見ると、アプリケーションストアのユーザーレビューは、短いテキストの断片として記述することができます。短文テキストは、ソーシャルメディア上のマイクロブログ（TweetやYouTube、Facebookのコメントなど）が爆発的に増加し、そのような大量の限られたテキストデータを分析するための効果的な方法が緊急に必要となった結果、近年、自然言語処理（NLP）研究において登場した新しいタイプのテキストです。このようなテキストは、語彙的にも意味的にも制限されていることが知られており、一般的には、口語的な用語（LOL、smh、idkなど）に加えて、音韻的な綴りやその他の新語が含まれています（Squires 2010）。この種のテキストでは，簡潔な要約を生成するために，抽出法がより効果的であることがわかっている(Nichols et al. 2012)．

抽出型テキスト要約アルゴリズムの大半は、単語の重要度を示す指標として、単語の頻度に依存している(Hahn and Mani 2000)。具体的には、人間が生成した要約に単語が現れる可能性は、その頻度と正の相関がある(Khabiri et al. 2011)。形式的には、抽出型要約プロセスは次のように説明されます。ユーザーレビューのリストRのトピックまたはフレーズMが与えられ、希望する要約の長さをKと仮定すると、カーディナリティの代表的なレビューのセットR(cid: ∀ri∈R(cid:4)、M∈ri、∀rj∈R(cid:4)、ri(cid:2)rjとなるようなKのカーディナリティを持つ代表レビューのセットを生成する。ri (cid:2) rjという条件は、サマリーに含まれる選択されたレビューが十分に異なる情報を提供する（すなわち、冗長ではない）ことを保証するために強制される（Inouye and Kalita 2011）。抽出されたサマリーは、ワードクラウドの形をとります。ワードクラウドは、重要な単語が大きなフォントサイズで書かれている（視覚化されている）、テキストデータの視覚的な表現と言えます。タグクラウドにおける単語の重要性は、テキスト中のその単語の頻度と単純に相関しています。図3は、Alexaアプリから抽出されたレビューのセットに対して生成されたワードクラウドです。このクラウドは、英語のストップワードを除去した後の、レビューで最も頻度の高い30の単語を示しています。

ワードクラウドは、レビューに含まれる主な懸念事項を捉えることができますが、文脈がないため、これらの懸念事項が実際に何であるかは不明であることが多いです。これに対して、全文抽出型サマリーには、文脈を保持できるという利点があります（Barker et al.2016; Khabiri et al.2011）。例えば、図3は、Alexaのレビューのセットで提起された2つの主要な問題に関連するレビューのサンプルです。これらの問題は、検索オプションの要求と、ホワイトスクリーンのバグの報告です。これらのレビューを完全に抽出することで、開発者はユーザーの主な関心事が実際に何であるかをよりよく知ることができます。

これらの観察に基づいて、我々の分析では、レビューの要約にいくつかのフルセンテンス抽出要約アルゴリズムを採用しています。これらのアルゴリズムは、短文要約タスクで多用されており、人間に近い要約を生成することが示されている(Erkan and Radev 2004; Inouye and Kalita 2011; Nenkova and Vanderwende 2005)。さらに、これらのアルゴリズムは理解と実装が容易であり、トピックモデリング（Chen et al. 2014）やクラスタベースの要約（Villarroel et al. 2016）などの他の技術よりも計算コストが低い。詳細には、我々の要約アルゴリズムは以下のように説明できる。

- ハイブリッド項頻度（TF）：ハイブリッドTFは、単語の基本頻度に依存して、特定の文章（ユーザーレビュー）のコレクションに対する重要性を決定します。形式的には、単語wiの重みは、レビューのコレクション全体における頻度(f wi)を、コレクション内のユニークな単語の数(N )で割ったものとして計算されます。古典的な単一文書のTFを改良（ハイブリッド）することで、コレクション全体で頻出する懸念事項を把握する必要があります（Inouye and Kalita 2011）。長さnワードのレビュー(rj )が要約に表示される確率は、その個々のワードの重みの平均として計算される。

- ハイブリッドTF.IDF: ハイブリッドTF.IDFは、ハイブリッドTF(Inouye and Kalita 2011)とコンセプトが似ています。ハイブリッドTF.IDFはハイブリッドTF(Inouye and Kalita 2011)に似ていますが、単語の逆文書頻度(IDF)を使用することで、すべてのユーザーレビューにおける単語の希少性を考慮しています。IDFは、テキスト内で頻度が高すぎる単語にペナルティを与えます。形式的には、TF.IDFは次のように計算されます。

ここで、TF(wi)は、コレクション全体における単語wiの項頻度であり、｜R｜は、コレクション内のレビューの総数であり、｜rj : wi ∈ rj ∧ rj ∈ R｜は、単語wiを含むR内のレビューの数である。そして、レビューの重要性は、その個々の単語の平均TF.IDFスコアとして計算できます。冗長性、つまり非常に類似した2つのユーザーレビューが要約に含まれる可能性を制御するために、トップスコアのレビューを要約に追加する前に、アルゴリズムは、そのレビューが要約に既に存在する他のレビューと一定の閾値を超えるテキストの類似性を持たないことを確認します。2つのレビューriとrjの間のテキスト上の類似性は、それらのベクトル間の角度の余弦として計算することができます。

- SumBasic: Nenkova and Vanderwende (2005)により導入されたSumBasicは、テキストコレクション内の単語の平均用語頻度(TF)を使用して、その値を決定します。しかし、個々の単語の重みは、冗長性を最小限にするために、要約に含まれた後に更新されます。このアプローチは以下のように説明できます。

1. サイズNの単語のコーパスにおいて、頻度f wiを持つ単語wiの確率を次のように計算する。

2. 2. 長さ n の単語を持つレビュー rj の重みは，その単語の平均確率として次のように計算される。

3. 3. トップスコアのレビューを選択し，サマリーに追加する．冗長性をコントロールするために、または同じトピックを同じ高頻度の単語を使用して記述しているレビューが選択される可能性を最小限にするために、選択されたレビューの各単語の確率は次のように減少します。

4. 4.必要な長さの要約が得られるまで、2から繰り返します。

ρ(wi)new = ρ(wi) × ρ(wi)

- LexRankです。LexRank はグラフベースのアルゴリズムで、与えられたコーパスの中で最も重要なセンテンスを決定するために使用されます。このアルゴリズムは、コーパス内のセンテンスの無向グラフを生成することで機能する(Erkan and Radev 2004)。グラフ内の個々のセンテンス（ノード）は、そのコサイン類似度を用いて接続される。グラフにはn×nのcosine-imilarity行列が作成される。重要でないリンクを除外するために、類似性行列に閾値を適用することができる。グラフ内の個々の文は、PageRank アルゴリズム（Page et al.1999）を使用してランク付けされる。形式的には、LexRankを使用して、ある文が要約に含まれる確率、すなわちp(u)は以下のように記述できる。

ここで、Nはドキュメント内のセンテンスの総数、dはダンピングファクターで、一般的には0.85として選択され(Brin and Page 1998)、uとvの間のTF.IDFコサイン類似度である(6)。この式を用いると、文のLexRankを計算する際に、リンクしている文のLexRankスコアにリンクの重みを乗じることで、文間の情報サブサンプションを考慮することができます（Erkan and Radev 2004）。

例題
以下の例は、画像共有アプリからサンプリングした4つのレビューを用いて、異なる要約アルゴリズムの動作を示しています。これらのレビューでは、主に2つのユーザーの関心事が提起されています。1つ目の懸念は、機能要求（レビューR2とR4）で、すべての写真を一度に表示する機能を求めています。2つ目はバグレポートで、写真を削除すると突然クラッシュするという問題が報告されています（レビューR1とR3）。

英語のストップワードを除去し、ステミングを適用した後、合計13のキーワードがサマライズアルゴリズムで考慮されるようになりました。表7は、これらの単語の頻度、ハイブリッドTF、ハイブリッドTF.IDFの重みを示しています。長さ2の要約が生成されると仮定すると（要約に含まれるレビューは2つだけ）、ハイブリッドTFはまず、ハイブリッドTFスコアの平均値が最も高いR3を選択する（表8）。次に、アルゴリズムはR2またはR4をランダムに選択します（どちらもリストの2番目にランクされます）。

ハイブリッドTF.IDFを使用すると、R1はハイブリッドTF.IDFの平均スコアが最も高いため、最初に要約に追加されます（表8）。アルゴリズムは2つ目の選択を行う前に、R1と他の3つのレビューのテキストの類似性(6)を計算します。R3はR1と3つの単語(crash, picture, delete)を共有しており、R1と最も似ています。そのため、要約には含まれていません。R2 と R4 は、R1 と同じテキストの類似性を持っています。アルゴリズムはそのうちの1つをランダムに選択します。

SumBasic を使用して、R3 はハイブリッド TF スコアの平均値が最も高いため、最初に選択されて要約に含まれます。選択後、単語(crash, delete, picture)のハイブリッドTFの重みは、それらを二乗することで減少します(9)。表7の5列目は、これらの単語のハイブリッドTF2を示している。個々のレビューの重みが再計算されます。その結果を表8の4列目に示します。現在、R2 と R4 の両方が R3 を削除した後のトップスコアのレビューであるため、アルゴリズムはそれらのうちの 1 つをランダムに選択してサマリーに含めます。LexRankを使用して、アルゴリズムはまず、TF.IDFコサイン類似度を使用して各2つの文の間の類似性を計算します(6)。図4は、結果として得られた類似性グラフである。ノード間の値は、センテンス内のコサイン類似度を示す。なお，TF.IDF(picture) = 0 となっていますが，これはすべてのレビューに現れているからです．各文のLexRankスコアは、(10)を用いて計算されます。初期のp(v)値は0.25に設定され、0.85のダンピングファクターが使用されます。アルゴリズムがR1からランダムウォークを開始したと仮定すると、最初の反復後の異なるレビューのLexRanksは表8に示されています。アルゴリズムは、最も高いLexRanksを持つR1とR2を選択することになります。

4.2 評価

要約アルゴリズムの評価は、通常、生成された要約の品質を人間が判断することに依存している(Lin and Hovy 2003)。例えば、複数の審査員が特定のテキストコレクションの異なる自動要約を提示し、その品質に基づいてこれらの要約をランク付けするよう求められる。また、自動生成された要約を人間が生成した要約（グランドトゥルース）と比較する評価方法もあります（Khabiri et al.2011）。今回の分析では，後者の方法を採用した．

評価を行うために，実験に参加してくれるプログラマーを8名募集しました．被験者は，平均4.3年のプログラミング経験を有しています．実験に使用したのは，Alexa，WellsFargo，Equifax，LinkedIn，FB Messenger，Dubsmashの各アプリです．各アプリについて、最新のレビュー500件を収集しました。レビューは2017年4月の第1週に集められた。これらのレビューをBOF+SVM分類器を用いて分類しました。次に、各アプリから100件の有益なレビュー（バグレポートまたは機能要求のいずれかに分類される）をランダムにサンプリングしました。そして、これらのレビューをランダムにして、4つの異なるバージョンを作成しました（100件のレビューは同じですが、順番が異なります）。このステップは、ランキングのバイアスを避けるために必要です（例えば、被験者は常にリストの上位からのレビューを好むでしょう）。ここで重要なことは，我々の分類器の精度が80%程度であることを考慮すると，サンプルされたレビューのごく一部には有益な情報が含まれていない（すなわち，有益であると誤って分類された）ということです．

各被験者には，3 つの異なるアプリのレビューをランダムに割り当て，各アプリのレビューのランダムなコピーを少なくとも 1 人の被験者が要約するようにしました．形式的には，被験者のセットを{s1, s2, ... . s8}とし、アプリのリストを{a, b, c, d, e, f }とし、各アプリαについて、4種類のランダム化されたレビューセットのリストを{α1, α2, α3, α4}とすると、アプリの被験者への割り当ては以下のようになります。

S1 = {F4, A4, C2}とします。
s2 = {a1, d3, c4}.
s3 = {d1, f3, b4}となります。
s4 = {b3, c1, f2} です。
S5＝｛D2，A2，E4｝とする。
s6 = {e3, b2, e1} です。
s7 = {f1, d4, c3} です。
S8＝｛A3，E2，B1｝としました。

被験者の主な仕事は，各レビューのセットを見て，そのセットで提起された最も重要な懸念を捉えていると思われるレビューを10個特定することでした．時間的な制約はありませんでした。しかし，ほとんどの被験者は2週間以内に回答しました．

次に，先に提案したさまざまな要約アルゴリズムを用いて，実験に参加した6つのアプリから抽出した100件のレビューを自動的に要約した．これらのレビューは，最初にステム処理と英語のストップワードの除去という前処理が行われた．この処理は，より正確な要約を生成するために必要です．例えば，一般的な英単語（the，could，theyなど）や，同じ単語の異なる形（crash，crashes，crashedなど）は，要約アルゴリズムの頻度計算に影響を与える可能性があります．

要約アルゴリズムの有効性を評価するために、各アプリについて、人間が作成した要約（リファレンス）と様々な自動生成された要約との間の平均用語重複を計算しました。この指標は、要約アルゴリズムの自動評価のために設計された一連の指標であるROUGEに基づいています(Lin 2004)。形式的には、要約アルゴリズムtの平均リコールは次のように計算できる。

ここで、Sは参照サマリーの数、match(t, si)は参照サマリーsiとtが生成したサマリーの両方に出現する用語の数、count (s i)は参照サマリーsiに含まれるユニークな用語の数である。自動化された要約は、参照要約からより多くの用語を含んでおり、より最適であると考えられる(Inouye and Kalita 2011; Lin 2004; Nenkova and Vanderwende 2005)。我々の分析では、異なる長さの要約（要約に含まれるレビュー数が10、15、20）についてリコールを測定した。図5は、異なる要約アルゴリズムの再現率を示しています。

さらに，レビューのBOF表現を用いて，異なる要約アルゴリズムの性能を評価した．具体的には，各アプリの各レビューの意味的フレーム表現を生成しました．そして，これらのレビューは，フレームの頻度に基づいて要約されました（単語の頻度ではなく，レビュー内のフレームの頻度が使用されます）．各アルゴリズムが、要約に含めるべき上位10件、15件、20件のレビューを選んだ後、これらのレビューのテキスト表現を再生成し、人間が生成した要約と比較した。BOFアプローチを用いた異なる要約アルゴリズムのリコールを図6に示す。

4.3 結果と考察

実験の最後に、被験者の要約行動を理解するために、被験者に簡単なインタビューを行った。8人の被験者のうち3人は，リストの上から下に向かってレビューを読み，2回目，3回目に問題が発生するたびにレビューを選択すると答えた．残りの5人は，まずレビューのリストを1，2回見て，主要な（最も頻度の高い）関心事を特定し，次にこれらの関心事をすべて捉えたレビューをランダムに選択したと答えた。すべての被験者は、ある問題の頻度が、その問題を要約に含めるかどうかの決定要因であると答えました。以下では、この結果をより詳細に説明します。

4.3.1 要約の結果

図5と図6は、データのBOW表現とBOF表現を用いた異なる要約アルゴリズムのパフォーマンスをそれぞれ示しています。さらに、表9は、BOF表現とBOW表現を使用して、各アプリのリコールの観点から、最もパフォーマンスの高い要約アルゴリズムを示しています。アルゴリズムの性能を比較するための実験的なベースラインとして、ランダムに生成したサマリーを使用しました。ランダムなベースラインは、抽出ベースの要約技術を評価するために一般的に使用されます(Inouye and Kalita 2011; Mackie et al. 2014)。基本的には、テキストのランダムな抽出により、要約アルゴリズムよりもまとまった要約が生成された場合、そのアルゴリズムはほとんど役に立たないと考えられます。

データの2つの表現の間の要約の質の差が統計的に有意であるかどうかをテストするために、2ウェイANalysis Of Variance（ANOVA）を実施します。データは、Kolmogorov-Smirnovの正規性の検定（p = 0.200）によって正規分布しており、ANOVAの正規性の仮定は満たされています。最初の独立変数は、データの表現（BOWとBOF）で、2番目の独立変数は、要約アルゴリズム（Random、Hybrid TF、Hybrid TF.IDF、SumBasic、LexRank）です。従属変数は、要約アルゴリズムのパフォーマンス（(11)で測定）である。

有意水準をα=0.05とすると，二元配置のANOVAテストの結果は，異なるアルゴリズム間でパフォーマンスに有意な差があることを示しています（F = 21.58, p < 0.01）．また，データ表現（BOF対BOW）の主効果が有意であることも分かりました（F = 6.37, p < 0.05）．また、データの表現方法と要約アルゴリズムの間には、有意な交互効果が検出されました（F = 4.92, p < 0.05）。特に、BOW表現では、要約アルゴリズムのパフォーマンスが著しく向上しました。

さらに、TukeyのHonest Significant Difference（HSD）テストを実行して、どのアルゴリズムが他のアルゴリズムよりも全体的に有意に良い結果を示したかを調べました（Tukey 1949）。Tukey's HSDは、ANOVAの後に実行することができるPost-Hoc分析で、特定のグループの平均値（お互いに比較した場合）がどのグループと異なるかを判定します。この検定は、平均のすべての可能なペアを比較します。表10の結果を見ると、データ表現にかかわらず、すべてのアルゴリズムがランダムベースラインを大幅に上回っています。また、SumBasicがHybrid TF（p < 0.01）とLexRank（p < 0.05）を有意に上回ったこともわかります。SumBasicはHybrid TF.IDFも上回っています。しかし、これらの2つのアルゴリズムの性能の差は、有意にはなりませんでした（p = 0.758）。

一般に、BOW表現では、SumBasicは人間が生成したサマリーで提起された懸念事項を最もうまく捉え、平均71%のリコールを達成しました。ハイブリッドTF.IDFも競争力があり、平均回収率は60%でした。ハイブリッドTF.IDFの最高のパフォーマンスは、類似性の閾値が0.2のときに達成されました。図7は、ハイブリッドTF.IDFの性能が、より大きなしきい値で悪化することを示しています（つまり、より多くの類似レビューがサマリーに許可されます）。一方、Hybrid TFは他のアルゴリズムと競争できず、包括的なサマリーを実現するためには冗長性の制御が重要であることを示唆しています。LexRank は、Hybrid TF をわずかに (有意ではない) 凌駕しましたが (p = 0.836)、SumBasic および Hybrid TF.IDF のパフォーマンスには及ばず、平均リコールは 41% でした。

4.3.2 生成されたサマリーの例

異なる要約アルゴリズムの性能を把握するために、Alexaアプリからサンプリングしたレビューのリストに対する性能を調べます。図8は，それぞれの要約アルゴリズムによって生成された要約（各10件）を示している．ユーザーの共通の関心事を示す単語がハイライトされている．長いレビューは、スペースを節約するために切り捨てられています。Alexaアプリのレビューリストを手作業で確認すると、最も頻繁に寄せられている懸念事項は、アプリのフリーズやクラッシュ、白い画面の問題などのバグ報告と、ランドスケープモード、検索オプション、拡張インターフェースなどの機能要求であることがわかります。また、頻度は低いですが、他のデバイスとのペアリングに関する問題や、システム設定に関する小さな問題などもあります。

図8は、Hybrid TFで生成されたサマリーに含まれる10件のレビューのうち、7件にユーザーの有効な懸念が含まれていることを示しています。しかし、これらの懸念は冗長で、アプリのクラッシュ、フリーズ、白い画面の問題などのバグについてのみ記述されていました。類似度のしきい値が0.2の場合、ハイブリッドTF.IDFはハイブリッドTFよりも成功しており、サマリーに含まれる10件のレビューのうち6件にユーザーの懸念が含まれていました。しかし、これらの懸念は、システムクラッシュ、フリーズ、白い画面の問題などのバグや、ランドスケープモードや検索オプションなどの要望を含む、より幅広い問題をカバーしていました。

SumBasicを使用した場合、要約に含まれる上位10件のレビューのうち、7件にユーザーの正当な懸念が含まれていました。これらの懸念は、クラッシュやフリーズのバグ、白い画面の問題、検索オプションやランドスケープモードの要望、その他のインターフェイスやユーザビリティの問題を含んでいました。LexRankの要約では、10件のレビューのうち9件に技術的なユーザーの意見が含まれていました。しかし、他のデバイスとのペアリングやシステム設定の問題など、あまり一般的ではない問題を捉えることができた一方で、LexRankは、検索オプションやランドスケープモードの要望など、ユーザーの主要な関心事を捉えることができませんでした。

一般的には、SumBasic と Hybrid TF.IDF が最も包括的なサマリーを生成し、被験者が特定した関心事の大部分を捉えることができました。しかし、ハイブリッドTF.IDFの最高のパフォーマンスは、冗長性制御の閾値を徹底的に較正した後にのみ達成されました。ハイブリッドTFは最も成功率が低く、レビューで最も頻繁に見られた懸念事項しか捉えられませんでした。一般的に、これらの懸念事項を説明するために使用される単語は、相対的な頻度が最も高い。したがって、これらの単語を含むレビューは、他の人気のある、しかし頻度の低い懸念事項を含むレビューよりも高いスコアになる傾向があります。LexRank は、SumBasic や Hybrid TF.IDF に比べて、より技術的に有用なレビューをサマリーに追加することができたにもかかわらず、成功率は低くなりました。これは、LexRank が長い文章を好む傾向にあることから説明できます (Otterbacher et al. 2009)。長い文章は短い文章よりも類似性グラフ（図4）の中心に位置する傾向があります。これは、LexRank が隣人に対する文の価値を考慮することに加えて、その文に対する隣人の重要性を考慮することに起因しています。したがって、より多くの単語を持つ長い文は、類似性グラフの中でより多くの文と強く結びついているため、短い文よりも評価されます。

4.3.3 BOFとBOWの要約の比較

また、BOF表現を用いた場合には、要約アルゴリズムの全体的な性能が大幅に低下することがわかりました（F = 4.92, p < 0.05）。一般的に、データのBOF表現を使用すると分類精度に良い影響を与える一方で、この表現を抽出的な要約に使用すると性能が低下するようです。これらの相反する結果は，異なるデータマイニングタスクで必要とされる抽象度に基づいて説明することができる．具体的には、レビューの分類では、レビューがバグレポートなのか機能要求なのかなど、データの一般的なカテゴリに関心があります。対照的に、要約タスクでは、より低いレベルの抽象度、つまり特定のユーザーの問題に関心があります。このようなシナリオでは、フレームセマンティクスを使用すると、情報が失われる可能性があります。以下の3つの例でこの問題を説明します。

(a) "I can't download my videos "というレビューの意味表現は、ダウンロードやビデオなどの単語がフレームを連想させないため、フレームCAPABILITYだけを持っています。

(b) "Can we get a gray filter?" と "Can we get a red font pls?!" という2つのレビューは、2つの異なる機能を要求しているにもかかわらず、以下のように同じフレームでアノテーションされているため、両方とも1つの問題とみなされました。

Can]CAPABILITY we [get]GETTING a [gray]COLOR filter?
Can]CAPABILITY we [get]GETTING a [red]color font pls?

(c)以下の3つのレビューにおいて。

-景観モードがなくなった。悪い点
- "アプリは全く動作せず、真っ白な画面が表示されます。"
- "リセットしたら、今度は何もできなくなった。"

一般的な単語であるbad、doesn't、won'tによって喚起されるINTENTIONALLY ACTのような支配的なフレームは、支配的な問題と勘違いされる傾向があり、要約アルゴリズムをミスリードしてしまいます。

以上の結果から、冗長性を制御した単純な周波数ベースの要約アルゴリズムは、人間の判断に大きく沿った要約を生成できることがわかった。BOF表現は、レビューの分類には役立つものの、要約アルゴリズムの性能を大きく低下させる可能性がある。したがって、実用上は、分類にはBOF表現、要約にはBOW表現を用いたツールが最も精度の高い結果を得られると考えられる。

5 ツールのサポート MARC 2.0

Jha and Mahmoud (2017a)で発表したMobile Application Review Classifier (MARC 1.0)のプロトタイプを拡張したMARC 2.0に、今回の分析で検討した分類・要約アルゴリズムを実装した。この実装9は、我々のアルゴリズムの計算上の実現可能性を示し、我々の分析の複製を容易にし、最終的にはアプリ開発者が日々のレビューマイニング作業で使用できるようにするために提供されています。以下、MARC 2.0の主な機能について説明します。

5.1 データ収集

MARC 2.0は、Apple App Storeから最新のレビューをダウンロードできるデータ収集機能をサポートしています。技術的には、MARC 2.0はアプリのiTunes IDを使って、App StoreのRSSフィードにウェブリクエストを行います。生成されたJSONページは、専用のパーサーで解析され、ユーザーレビューが抽出されます。アプリのID番号は、iTunes上のアプリのURLから直接取得できます。例えば、GmailのID番号（422689480）は、以下のようにiTunesのページから直接取得することができます。

https://itunes.apple.com/us/app/gmail-email-by-google /id422689480?mt=8

アプリのID番号が提供されると、MARC 2.0は次のようなWebリクエストを行います。

https://itunes.apple.com/rss/customerreviews/page=1/ id=422689480/sortby=mostrecent/json

抽出されたレビューは、MARC 2.0のホームページ上でユーザーに表示されます。

5.2 分類

MARC 2.0の分類エンジンは、現在Naive Bayes (NB)とSupport Vector Machines (SVM)をサポートしています。これら2つの分類器は、WekaのAPIを通じて実装されています。このAPIは、入力されたレビューをWeka互換のファイル形式(.arff)に変換します。フィルタStringToWordVectorは、分類されるレビューの単語×文書マトリックスを生成するために使用されます。MARC 2.0は、分類エンジンのトレーニングとテストのために、手動で分類されたレビューのデフォルトトレーニングデータセットを使用しています。このデータセットは、我々の実験的分析で使用されたデータセットからコンパイルされています。さらに、ユーザーは独自のトレーニングデータセットを提供することができます。柔軟性を確保するため、MARC 2.0では、ステム処理やストップワードの除去など、複数のテキスト前処理の設定を選択することができます。ステム処理は、WekaのIteratedLovinsStemmer (Lovins 1968)を使用して実装されており、ストップワードは、ユーザーが編集できる別の設定ファイルで提供されます。さらに、MARC 2.0では、分類のためのデータ表現（BOW vs. BOF）を選択することができます。BOF表現は、確率フレームセマンティックパーサーSEMAFOR(Das et al. 2010)によって生成されたXMLファイルを読み込んで解析する特別目的のパーサーによってサポートされています。図2は，MARC 2.0 が入力レビューの BOF 表現を生成するのに必要な平均時間を示している．

5.3 サマリー化

MARC 2.0は、分類されたユーザーレビューを要約するオプションをユーザーに提供します。MARC 2.0の要約エンジンは、我々の分析で評価された様々な要約アルゴリズム(Hybrid TF, Hybrid TF.IDF, SumBasic, and LexRank)をサポートしています。ユーザーは要約のサイズ(要約に含まれるレビュー数)やHybrid TF.IDFの閾値を選択することができます。MARC 2.0では、レビューのワードクラウドサマリーを生成することもできます。ユーザーは、2つのビューの間を行き来することができます。

5.4 ストップワード・エディター

MARC 2.0では、ユーザーがストップワードのリストを編集する機能（単語の追加と削除）を提供しています。我々の分析によると、生成された要約の品質は、無関係な単語や、アプリ開発者に有益な情報を提供しない英語のストップワードによって、深刻な影響を受ける可能性があることがわかりました。例えば、アプリの名前はレビューに頻繁に登場する傾向があり、要約アルゴリズムの頻度計算に影響を与えます。このような単語は、一般的な英語のストップワードのリストには現れません。そのため、このような単語をフィルタリングする機能をユーザーに提供する必要があります。

6 関連研究

アプリストアのレビュー分析に関する研究は、ここ数年で顕著に進んでいます。レビューの分類、要約、優先順位付けについて、多くの研究が行われています。これらの研究の包括的な調査は、Martin et al.（2017）で提供されています。このセクションでは、このドメインにおける重要な関連研究を選択的にレビューし、議論する。我々のレビューには、ソフトウェアのユーザーフィードバックのためのソーシャルメディアプラットフォーム（Twitter）のマイニングのドメインからの重要な仕事も含まれています。

6.1 ユーザーフィードバックのためのApp Storeレビューのマイニング

Iacob and Harrison (2013) は、アプリストアのレビューから機能要求をマイニングするツールであるMARAを紹介しています。MARAは、事前に定義された言語ルールに基づいて、機能要求を表現する文章を識別します。これらのルールは、機能要求に関連するキーワードや言語パターンを分析することで特定されました。MARAは、Google Playから抽出した480件のレビューを用いて評価されました。その結果、レビューの23.3%が機能要求を表していることがわかりました。

Carre´no and Winbladh (2013) は、トピックモデリングとセンチメント分析による分類を適用して、要件変更に関連するユーザーコメントを特定しました。具体的には、ユーザーのコメントを処理して、言及された主なトピックと、それらのトピックを代表するいくつかの文章を抽出しました。提案手法を、手動で分類されたユーザーレビューの3つのデータセットで評価したところ、精度と省力化の点で有望なパフォーマンスレベルが示された。

Guzman and Maalej (2014) は，開発者がアプリのレビューをフィルタリング，集約，分析するための自動化アプローチを提案した．提案されたアプローチでは、コロケーション発見アルゴリズムを使用して、レビューに記載されている細かい要件を抽出します。これらの要件は、トピックモデリングを使用して、より意味のある高レベルの特徴にグループ化されます。著者らは、7つのiOSおよびAndroidアプリから抽出した32,210件以上のレビューを用いて分析を行いました。その結果、提案されたアプローチは、レビューに記載されている最も一般的な機能要求をうまく捉え、グループ化することができました。

Chenら（2014）は、アプリの開発者が最も有益なユーザーレビューを特定するための計算フレームワークであるAR-Minerを発表しました。AR-Minerは、半教師付きテキスト分類アルゴリズムであるExpectation Maximization for Naive Bayesを使用して、情報量の少ないレビューをフィルタリングします。残ったレビューは、トピックモデリング（Blei et al.2003）を用いて分析され、異なるグループに分類されました。これらのグループは、潜在的な情報価値に基づいて、レビューランキングスキームによってランク付けされました。提案されたアプローチは，人気のある4つのAndroidアプリから収集したアプリのレビューを手動で分類したデータセットで評価された．その結果、精度、リコール、ランキングの質の面で高い精度レベルを示した。

Panichellaら（2015）は，モバイルアプリのレビューを技術的なフィードバック（バグレポートや機能要求など）のいくつかのカテゴリに分類するための教師付きアプローチを提案した．著者らは、各レビューから、最も重要な単語、レビューの主な感情、メンテナンス要求の可能性を示す言語パターンなど、一連の言語的特徴を抽出しました。そして、これらの特徴の様々な組み合わせを用いて、異なるタイプの分類器を学習しました。その結果、言語パターンとセンチメントスコアを反復して学習した決定木（Quinlan 1986）が、精度とリコールの点で最高のパフォーマンスを達成しました。

Maalej and Nabil (2015) は、アプリのレビューをバグレポート、機能要求、ユーザー体験、評価に分類するためのいくつかの確率論的手法を紹介しました。著者らは、Naive Bayes、Decision Trees、Maximum Entropyなど、いくつかのバイナリクラスおよびマルチクラスの分類器を用いて実験を行った。Google PlayとApple App Storeから手動でラベル付けされた4400件のレビューを集めたデータセットを用いて、これらの異なる分類器の性能を評価した。その結果，バイナリ分類器（Naive Bayes）の方がマルチクラス分類器よりもレビュータイプを予測する精度が高いことが分かりました。また、星評価、時制、センチメントスコア、長さなどのレビューの特徴と、ステミングやレマタイト化などのテキスト解析技術が、分類の精度を高めることも明らかになりました。

Khalidら（2015）は、開発者がユーザーからの苦情の可能性をより適切に予測し、優先順位をつけることを主な目的として、ユーザーレビューの分析研究を行いました。著者らは、20のiOSアプリの数千件のアプリレビューを手作業で調査し、1つ星と2つ星のレビューを中心に分類しました。分析の結果、12種類の一般的なユーザーの不満が明らかになり、その中でも機能的なエラーが最も頻繁に発生していることがわかりました。

Mcllroyら（2016）は、人気のアプリストアにおけるユーザーレビューのマルチラベル性を分析しました。データの質的分析により、かなりの量（30％）のユーザーレビューが、複数の問題タイプ（機能要望、機能的不満、プライバシー問題）を提起していることがわかりました。著者らは、レビューに複数のラベルを自動的に付与するために、いくつかの分類およびマルチラベリングの手法を試しました。その結果、Pruned Sets with threshold extension (PSt) (Read et al. 2008)とSVMの組み合わせが最高の性能を達成しました。

Villarroelら（2016）は、CLAP（Crowd Listener for releAse Planning）を導入しました。CLAPは、ユーザーレビューを分類して優先順位をつけ、リリース計画を支援します。技術的には、著者らは、関連するレビューのクラスタを発見するための密度ベースのアルゴリズムであるDBSCAN（Ester et al.1996）を使用しました。また、ランダムフォレストアルゴリズムを用いて、クラスタのサイズ、平均レビュー評価、レビューで言及されているハードウェアデバイスなどの要素に基づいて、各クラスタに高優先度または低優先度のラベルを付けました。CLAPは、産業界の環境と専門家の判断を用いて評価されました。その結果、CLAPはレビューを正確に分類してクラスタ化し、意味のある発売計画の提案を行うことができることがわかりました。

Ciurumeleaら（2017）は、より良いリリース計画のためにモバイルアプリのレビューとコードを分析するための分類法を提案した。著者らは、ソフトウェアのメンテナンス時に開発者にとって関連性の高いユーザーレビューのモバイル特有のカテゴリーを定義した（互換性、使用方法、リソース、価格、苦情など）。そして，機械学習（ML）と情報検索（IR）の技術を用いたプロトタイプを導入し，レビューを分類し，レビューで提起された問題を処理するために修正される可能性の高いソースコードファイルを推奨した．提案されたアプローチは、Google Playの39のオープンソースアプリを用いて評価された。その結果、提案されたアプローチは、事前に定義された分類法に従ってレビューをきちんとした精度で整理できることがわかりました。

Groenら（2017）は、アプリのレビューからユーザーの品質に関する懸念（非機能要件）をマイニングすることを研究しました。著者らは、オンラインレビューにタグ付けすることで、ユーザーが主に操作性や信頼性に関する懸念を表明していることを発見し、操作性、適応性、耐障害性、相互運用性などの側面に注目しました。さらに著者らは，ユーザレビューに含まれるユーザビリティに関する懸念を自動的に捉えるための言語パターンを提案した．大規模なレビューのデータセットを用いてこれらのパターンを評価したところ，ユーザの品質に関する懸念に関する記述を高い精度で特定できることがわかった．しかし、リコールレベルは非常に低いことが報告されている。

Johannら（2017）は、アプリの機能を参照するためにレビューテキストで頻繁に使用される18の品詞パターン（例：動詞-名詞-名詞）と5つの文パターン（例：列挙と接続詞）を特定しました。このようなパターンを用いることで，SVMやNBなどの分類アルゴリズムに比べて，大規模な学習データや設定データを必要としないことが大きな利点となる．提案したアプローチは，10種類のアプリから抽出したレビューを用いて評価した．その結果，言語パターンは，センチメント分析やトピックモデリングなど，より計算コストの高い技術に依存する他のモデルよりも優れていることがわかった．

要約すると，アプリのユーザレビューを分類するために，古典的なテキスト分類技術（SVMとNB）が頻繁に使用されていることがわかった．場合によっては、ユーザーの技術的要求を捕捉するための直接的かつ教師なしのアプローチとして、言語パターンが使用されます（Groen et al. 2017; Ha and Wagner 2013; Johann et al. 2017; Panichella et al. このような手法は、「[誰か]は[動詞]を試してみるべきだ」といった、レビューからの再帰的な構文パターンのマイニングに依存しています。本稿での我々の作業は、これら2つのアプローチの組み合わせとして認識することができます。具体的には，まずレビューを言語的なフレーム表現に変換します．そして、これらの表現は、SVMやNBなどのアルゴリズムを用いて分類されます。言語的パターンではなく、意味的フレームを使用する利点は、パターンの完全なリストを準備することが、手間と時間のかかるプロセスであるという事実に起因する。特に、何百ものレビューを手作業で調査し、パターンを抽出しなければなりません（Iacob and Harrison 2013）。その結果、このアプローチは、SVMやNBなどのテキスト分類アルゴリズムと比較して、非常に低いリコールレベルを達成する傾向があります（Groen et al.2017; Panichella et al.2015）。しかし、セマンティックフレームアプローチは、200,000以上の総アノテーションセットのデータベースに依存しているため10、レビューに現れる可能性のあるより多くの言語構造を考慮することができます。

要約の観点から、我々の文献レビューによると、既存の研究の大部分は、Latent Dirichlet Allocation (LDA) (Blei et al. 2003) などのトピックモデリング技術に依存し、局所的に関連するレビューのクラスタを作成しています (Carre´no and Winbladh 2013; Chen et al. 2014; Guzman and Maalej 2014)。しかし、最先端のトピックモデリング技術のほとんどは、意味のある結果を生成するために、いくつかのパラメータを徹底的に較正する必要があります(Blei et al. 2003)。さらに、生成されたトピックは、解釈や合理化が容易ではないことが多く、大量のトピック（100～200）を調べ上げることは、網羅的でエラーが発生しやすいプロセスとなります（Carre´no and Winbladh 2013）。また、他のクラスタリング技術（DBSCANなど）では、意味のあるクラスタを生成するために、いくつかのパラメータの値を事前に設定する必要があります（Ciurumelea et al.2017）。このレベルの操作の複雑さは、これらの技術の上に構築されたあらゆるツールの実用性を制限する（Lo et al. 我々の分析では、周波数ベースの要約技術を使用することで、この複雑さを回避する。これらの技術は、ユーザーによる校正を必要とせず、トピックモデリングやクラスタリング技術と比較して計算量が非常に少ないため、実用的なプロトタイプに簡単に統合することができます。

6.2 ソフトウェア・ユーザー・フィードバックのためのTwitterフィードのマイニング

Twitterは、ソフトウェアのエンドユーザが、ソフトウェアシステムに関する経験や懸念をマイクロブログの形で公に共有することを可能にします。アプリストアで提供されているユーザーフィードバックと同様に，Twitterのデータを収集・分類することで，ソフトウェア開発者がユーザーのニーズを推測したり，コードのバグを検出したり，システムの将来のリリースを計画したりするのに役立ちます．例えば、私たちの以前の研究（Williams and Mahmoud 2017）では、ソフトウェア開発者が恩恵を受けることができる有用なユーザーの技術的フィードバックのソースとして、Twitterを調査しました。我々の分析は、10のソフトウェアシステムのTwitterフィードから収集した4,000のツイートを用いて行われました。その結果、収集したツイートの約50％に実用的なメンテナンス要求が含まれていることが判明しました。また、SVMやNBなどのテキスト分類器は、技術的に有益なツイートを捕捉して分類するのに非常に有効であることが分かりました。さらに、ソフトウェア関連のツイートを要約する際に、Hybrid TF、Hybrid TF.IDF、SumBasicの性能を評価しました。その結果、本稿の結果と同様に、SumBasicは人間が生成した要約と最も高い一致率を達成することができました。

Guzmanら（2016）は、ソフトウェア関連のツイートの利用特性と内容を明らかにするために、1,000件のツイートサンプルを手動で分析・分類しました。分析の結果、ソフトウェアのツイートには、技術者と非技術者の異なるグループの利害関係者にとって有益な情報が含まれていることが分かりました。また、著者らは自動分類を用いて、技術的な利害関係者に関連するツイートを特定しました。その結果、有用なツイートを分離するのに、SVMが決定木をわずかに上回った。より最近の研究では、Guzmanら（2017）が、ソフトウェアアプリケーションに関するツイートを自動的に分類、グループ化、およびランク付けするアプローチであるALERTmeを発表した。著者らは、Multinomial NBを使用して、ソフトウェアの改善要求を持つツイートを分類しました。その後、トピックモデリングを用いて、意味的に関連するツイートをグループ化しました。そして、トップレベルで関連するツイートのグループは、様々な属性の加重関数を使用して、知覚された重要性に基づいてランク付けされました。ALERTmeは、複数のソフトウェアアプリケーションのTwitterフィードから収集した68,108件のツイートを用いて評価されました。その結果、まずまずの精度が得られました。また、この結果は、Twitterがエンドユーザーの要求を含むソフトウェアの進化に関連する情報の重要なソースになり得ることを示した。

Nayebiら（2017）は、70のモバイルアプリのTwitterフィードとソフトウェアレビューを分析した。著者らはNBを用いて、そのデータをバグレポートと機能要求に自動的に分類した。その後、トピックモデリング（LDA）を用いて、関連する懸念事項のグループを生成した。その結果、Twitterはモバイルアプリ開発を支援するための補完的な情報を提供することができ、Twitterのみで発見された機能要求は約22％、バグ報告は約13％多かった。

一般的には、アプリ開発者が技術的なフィードバックを得るための補完的なコミュニケーションチャネルとして、Twitterを活用できることが示唆されています。技術的には、ツイートがユーザーレビューといくつかの意味的・語彙的属性を共有していることが示唆されています。そのため、アプリストアのレビュー分析で効果が実証されている分類（NBやSVMなど）や要約（LDAやSumBasicなど）の技術は、特定の制限に対処すれば、Twitterデータにも適用できます。例えば、Twitterのメッセージは、ユーザーレビューよりも本質的に短いものです。このことは、LDAのようなデータ量の多い手法の有効性に悪影響を与える可能性があります。しかし、この問題は、より長いテキストを生成するために複数のツイートをグループ化するアシストLDAなどの技術を使用することで解決できます(Mehrotra et al. 2013)。さらに、Twitterのフィードには、無関係な情報やスパムの割合が多いことがあります（Wang 2010）。これは、データの分類や要約技術の精度に影響を与える可能性があります。この問題に対処するために、スパムフィルタリング（McCord and Chuah 2011）やスマートデータ収集技術を使用して、そのような無関係なデータをフィルタリングすることができます（例えば、ソフトウェアシステムのTwitterアカウントに直接宛てられたツイートのみを考慮するなど）（Williams and Mahmoud 2017）。

7 妥当性を脅かすもの

本稿で紹介した分析には、いくつかの限界があります。以下では、本研究の潜在的な内的妥当性、外的妥当性、構成的妥当性の脅威を、我々の緩和戦略とともに説明する。

7.1 内部的妥当性

内部的妥当性とは、実験で確立された因果関係に影響を与える可能性のある交絡因子のことを指します(Dean and Voss 1999)。本研究の内部的妥当性に対する潜在的な脅威は、グランドトゥルースデータセットの作成に人間の判断が用いられたという事実です。さらに、参考文献の要約を作成するために、人間の専門家が使用された。人間の判断は主観的になりがちなので、これは実験的バイアスになるかもしれません。しかし，テキストの分類作業では，人間が手作業でデータを分類することは珍しくありません．同様に，機械で生成された要約を人間が生成した要約と比較して評価することは，標準的な評価手順です．最終的には、人間がサマリーの利用者となるため、サマリーの品質やまとまりを判断するのは人間です。人間を使うことによる主観性や偏りの脅威は避けられませんが、複数の判定者を使ってデータを分類・要約することで、部分的に軽減することができます。我々の分析では、データは3人の異なる審査員によって分類され、専門性の異なる8人の専門家によって要約された。

データの要約に使用した人間の専門家のうち4人が大学院生であったことが脅威となっているかもしれません。しかし，大学院生の被験者全員が何らかの産業界での経験（平均2年）を報告していることから，この脅威の影響は最小限に抑えられたと考えています．実際、実験ベースのソフトウェア工学研究における既存の証拠は、産業界の専門家と大学院生との間の差が無視できるほど小さいことを示唆しています（Runeson 2003）。

7.2 外部妥当性

外部妥当性への脅威は、結果の一般化に影響を与える(Dean and Voss 1999)。特に、私たちの実験の結果は、私たちの特定の実験設定を超えて一般化しないかもしれません。外部妥当性に対する潜在的な脅威は、分類と要約の実験に使用したデータセットに起因する。特に、我々のデータセットはサイズが限られており、限られた数のアプリから生成されたものです。この脅威を軽減するために、私たちは、文献で使用されたことのある2つの外部データセットと、私たちが収集したローカルデータセットを含む、複数のソースからデータセットをコンパイルしました。また、レビューは、広範なアプリケーション・ドメインにまたがる多様なアプリから選択するようにしました。

7.3 構成の妥当性

7.3 構成の妥当性 構成の妥当性とは，様々なパフォーマンス指標が，それらが測定しようとしている概念をどの程度正確に捉えているかということである (Dean and Voss 1999)．今回の実験では，関連研究で広く使用されている標準的な性能指標（Recall，Precision，F1）を使用して分類技術の性能を評価したため，構成概念の妥当性に対する脅威は最小限に抑えられました．さらに，抽出型要約アルゴリズムの評価には，要約の自動評価のために設計された指標群のベンチマークであるROUGE（Lin 2004）に基づく指標を使用した．これらの指標は，我々が関心を持っていたパフォーマンスのさまざまな側面を十分に定量化していると考えられる．

8 結論と今後の課題

モバイルアプリケーションストアのユーザーレビューは、アプリ開発者にとって技術情報の豊富な情報源です。このような情報を利用することで、適応性と応答性に優れたリリース計画プロセスを実現することができます。本研究では，アプリストアのユーザレビューを分類・要約するための，意味的に認識された新しいアプローチの性能を調査した．提案されたアプローチは、意味的役割のラベリングに基づいています。具体的には，個々のユーザレビュー文を抽出してアノテーションを行い，各文に登場する単語が果たす意味的役割を特定する．このような役割は、セマンティックフレームと呼ばれ、レビューの基本的な意味を表しています。主な前提として、テキストの意味に依存することで、データマイニングアルゴリズムの予測能力を高めることができます。

分析を行うために、ユーザーレビューの実験データセットを複数のソースからまとめました（Chen et al. 2014; Maalej and Nabil 2015; Jha and Mahmoud 2017b）。個々のレビューは、FrameNetを用いて意味的にアノテーションされました。そして、Bag-of-Frame（BOF）として表現されたアノテーションされた文章を、Naive Bayes（NB）とSupport Vector Machines（SVM）を用いて分類した。その結果，Bag-of-Frame (BOF) 方式は，Bag-of-Words (BOW) 方式と比較して，競争力のある結果を得ることができた．しかし，テキストのBOF表現を用いて学習した分類器は，未見のレビューを集めたテストセットに対してよりよく一般化することができ，BOW分類モデルがオーバーフィッティングに陥っていることが示唆された．BOW法に対するBOF法の主な利点は，分類に必要な特徴の数が大幅に減少したことにある．フレームと単語の数が少ないほど、低次元のモデルが生成され、より正確な予測が可能になります。

さらに、より効果的なデータ探索プロセスを行うために、技術的に有益なユーザーレビューを複数の要約アルゴリズム（ハイブリッドTF、ハイブリッドTF.IDF、SumBasic、LexRank）を用いて要約しました。これらのアルゴリズムは、ソーシャルメディアのデータを扱う上で、そのシンプルさと有効性で知られています。これらのアルゴリズムの性能を評価するために、8人のプログラマーを使った人体実験を行いました。具体的には、異なる要約アルゴリズムによって生成されたレビューの要約を、人間が生成した要約と比較しました。その結果、冗長性制御を用いた頻度ベースの要約アルゴリズムであるSumBasicは、人間の判断に大きく沿った要約を生成できることがわかった。また、レビューの意味表現（BOF）を利用すると、情報が失われ、代表性の低い要約が生成されることが示されました。最後に、本論文の研究ラインは、今後の研究で追求すべき、以下のようないくつかの研究の方向性を示しました。

- 分析。
今後の研究では、他のデータ分類および要約アルゴリズムを調査する予定です。我々の目的は、適度な計算量と空間的な複雑さを維持しながら、高い精度レベルを達成できるこれらのアルゴリズムの組み合わせを特定することである。

- ユーザー調査。
複数のユーザー調査を実施し、提案する手法やツールの使用感を評価するとともに、既存のアプリレビュー分類・要約ツール（AR-Miner (Chen et al. 2014)など）との性能比較を行います。

- アプリ市場の分析
今後の研究の主要な部分は、アプリ市場でのイノベーションと生き残りをサポートするために、我々の発見を活用することに焦点を当てています。この目標は、ユーザーからのフィードバックの他の情報源（例えば、Twitter（Guzman et al. 2016; Williams and Mahmoud 2017））とアプリストアのレビューを統合することで、ユーザーの期待とニーズを包括的に理解し、機能アップデートがアプリのユーザー獲得率と継続率に与える影響を予測することで達成されるでしょう。